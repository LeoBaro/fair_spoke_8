infrastructure:
  num_workers: 8 
  enable_metrics: true
  logging_level: DEBUG
  save_npy: true
  apply_filters: true

unimodal:
  # ['same_input',  'classic']
  apply_filters: same_input 
  batch_size: 4000

  caption_min_words: 2
  caption_max_length: 100
  caption_max_chars: 5

  lang_detection_model_path: models/lid.176.bin
  lang_detection_score_threshold: 0.7
  lang_detection_language: en

  tagging_model_path: en_core_web_trf
  good_captions_pos_distribution_path: models/common_pos_patterns.txt

  image_min_aspect_ratio: 0.8
  image_max_aspect_ratio: 1.8

  specificity_threshold: 0.5           
  curvature: 1.0    

multimodal:
  batch_size: 32
  clip_processor: "openai/clip-vit-base-patch32"
  clip_model: "openai/clip-vit-base-patch32"
  caption_generation_processor: "llava-hf/llava-v1.6-mistral-7b-hf"
  caption_generator_model: "llava-hf/llava-v1.6-mistral-7b-hf"
  diffusion_model: ""
  clip_score_threshold: 0.5