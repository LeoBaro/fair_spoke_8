{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import time\n",
    "import random\n",
    "import spacy\n",
    "import cupy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = [\n",
    "    'similarity',\n",
    "    'hash',\n",
    "    'punsafe',\n",
    "    'pwatermark',\n",
    "    'LANGUAGE',\n",
    "    'caption',\n",
    "    'url',\n",
    "    'key',\n",
    "    'status',\n",
    "    'error_message',\n",
    "    'width',\n",
    "    'height',\n",
    "    'original_width',\n",
    "    'original_height',\n",
    "]\n",
    "\n",
    "# splits = {\n",
    "#     'train': 'data/train-*-of-*.parquet', \n",
    "#     'test': 'data/test-00000-of-00001-f5aa494af1d25f74.parquet'\n",
    "# }\n",
    "\n",
    "# df_train = pl.read_parquet(\n",
    "#     'hf://datasets/yuvalkirstain/laion-hd-subset/' + splits['train']\n",
    "# )[subset]\n",
    "\n",
    "# df_test = pl.read_parquet(\n",
    "#     'hf://datasets/yuvalkirstain/laion-hd-subset/' + splits['test']\n",
    "# )[subset]\n",
    "\n",
    "df_train = pl.read_parquet('data/laion-hd-subset-train')[subset]\n",
    "df_test = pl.read_parquet('data/laion-hd-subset-test')[subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning Function\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def clean_text(text):\n",
    "    # 1) Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2) Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # 2.5) Remove non-ascii characters\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "    \n",
    "    # 3) Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # # 4) Tokenize\n",
    "    # tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # # 5) Remove stopwords\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # tokens = [w for w in tokens if w not in stop_words]\n",
    "    \n",
    "    # # 6) Lemmatize\n",
    "    # lemmatizer = WordNetLemmatizer()\n",
    "    # tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    \n",
    "    tokens = text.split()\n",
    "    # # 7) Remove extra whitespace / rejoin if desired\n",
    "    cleaned_text = \" \".join(tokens)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# # Example usage\n",
    "# raw_text = \"Hello there!!! This is a sample text, containing numbers like 123 and punctuation.\"\n",
    "# cleaned = clean_text(raw_text)\n",
    "# print(cleaned)\n",
    "# # Output might be: \"hello sample text containing number like punctuation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = random.randint(0, df_train.shape[0])\n",
    "\n",
    "caption = df_train.with_row_index().filter(\n",
    "                    pl.col('index') == row_index\n",
    "                )['caption'].to_list()[0]\n",
    "\n",
    "cleaned_caption = clean_text(caption)\n",
    "\n",
    "print(\n",
    "    f\"Original Caption:\\n{caption}\\n\\n\"\n",
    "    f\"Cleaned Caption:\\n{cleaned_caption}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are uids unique?\n",
    "\n",
    "df_train['hash'].unique().count() == df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are Captions unique?\n",
    "\n",
    "df_train['caption'].unique().count() == df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = df_train['caption'].is_duplicated()\n",
    "\n",
    "df_train.filter(\n",
    "    condition\n",
    ").sort('caption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.unique(\n",
    "        subset='caption',\n",
    "        keep='first',\n",
    "        maintain_order=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are Captions unique?\n",
    "\n",
    "df_train['caption'].unique().count() == df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are urls unique?\n",
    "\n",
    "df_train['url'].unique().count() == df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['LANGUAGE'].value_counts().sort('count', descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is English Caption?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caption Filtering by Language\n",
    "\n",
    "import fasttext\n",
    "import os\n",
    "import re\n",
    "\n",
    "def fasttext_load_model(model_name: str, dir_name: str, cwd: str):\n",
    "    try:\n",
    "        model_path = os.path.join(cwd, dir_name, model_name)\n",
    "        model = fasttext.load_model(model_path)\n",
    "    except:\n",
    "        parent_dir = os.path.dirname(cwd)\n",
    "        return fasttext_load_model(model_name, dir_name, parent_dir)\n",
    "    else:\n",
    "        return model\n",
    "    \n",
    "def extract_str_from_pattern(s: str, regex: str) -> str:\n",
    "    pattern = re.compile(regex)\n",
    "    match = pattern.match(s)\n",
    "    tot_groups = len(match.groups())\n",
    "    matched_groups = match.groups(tot_groups)\n",
    "    return matched_groups\n",
    "\n",
    "def is_english_sentence(sentence: str):\n",
    "    predictions, score = model.predict(sentence.strip().replace('\\n', ' '))\n",
    "    return {\n",
    "        \"is_english\":predictions[0] == '__label__en', \n",
    "        \"lang_detected\":predictions[0], \n",
    "        \"score\":score[0]\n",
    "        }\n",
    "\n",
    "model = fasttext_load_model('lid.176.bin', 'models', os.getcwd())\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     model = fasttext_load_model('lid.176.bin', 'models', os.getcwd()) \n",
    "#     input_file = \"/home/fbernardi/Documents/fair_spoke_8/train_cap.txt\"\n",
    "#     with open(input_file, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             result = is_english_sentence(line)\n",
    "#             detected_lang = extract_str_from_pattern(result['lang_detected'], r'^__label__(.+)$')\n",
    "#             print(detected_lang[0], result)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Prediction\n",
    "start_time = time.time()\n",
    "list_of_captions = df_train['caption'].to_list()\n",
    "list_of_captions = [x.strip().replace('\\n', ' ') for x in list_of_captions]\n",
    "\n",
    "lang, scores = model.predict(list_of_captions)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "tot_elapsed_time = (elapsed_time/df_train.shape[0]) * 4e6\n",
    "print(f\"Esitmated Elapsing Time for 4M rows: {tot_elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_data_types = pl.Struct({\n",
    "                        pl.Field(\"is_english\", pl.Boolean),\n",
    "                        pl.Field(\"lang_detected\", pl.String),\n",
    "                        pl.Field(\"score\", pl.Float64)\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_train = df_train.drop(['is_english', 'lang_detected', 'score'])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_train = df_train.with_columns(\n",
    "    df_train['caption'].map_elements(\n",
    "        lambda x: is_english_sentence(x), \n",
    "        return_dtype=map_data_types, \n",
    "        skip_nulls=True)\n",
    "        .alias('lang_detection')\n",
    ").unnest(\"lang_detection\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "tot_elapsed_time = (elapsed_time/df_train.shape[0]) * 4e6\n",
    "print(f\"Esitmated Elapsing Time for 4M rows: {tot_elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.with_columns(\n",
    "        df_train['lang_detected'].map_elements(\n",
    "        lambda x: extract_str_from_pattern(x, r'^__label__(.+)$')[0], \n",
    "        return_dtype=pl.String, \n",
    "        skip_nulls=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = make_subplots(\n",
    "                    rows=2, cols=1,\n",
    "                    shared_xaxes=True,\n",
    "                    vertical_spacing=0.03,\n",
    "                    subplot_titles=[\"Dataset Language\", \"Language Detected\"]\n",
    "                    )\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=df_train['LANGUAGE']\n",
    "            .value_counts()\n",
    "            .sort('count', descending=False)['count']\n",
    "            .to_list(),\n",
    "\n",
    "        y=df_train['LANGUAGE']\n",
    "            .value_counts()\n",
    "            .sort('count', descending=False)['LANGUAGE']\n",
    "            .to_list(),\n",
    "\n",
    "        orientation='h'\n",
    "        ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=df_train['lang_detected']\n",
    "            .value_counts()\n",
    "            .sort('count', descending=False)['count']\n",
    "            .to_list(),\n",
    "\n",
    "        y=df_train['lang_detected']\n",
    "            .value_counts()\n",
    "            .sort('count', descending=False)['lang_detected']\n",
    "            .to_list(),\n",
    "            \n",
    "        orientation='h'\n",
    "        ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, width=1000, title_text=\"Distributions\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config(fmt_str_lengths=1000, tbl_width_chars=1000)\n",
    "\n",
    "# Same Detected Language\n",
    "\n",
    "df_train.filter(\n",
    "    (pl.col('LANGUAGE') == pl.col('lang_detected'))\n",
    ").select(['LANGUAGE', 'lang_detected', 'caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only Detected Language (No Ground Truth)\n",
    "\n",
    "df_train.filter(\n",
    "    (pl.col('LANGUAGE') == \"nolang\")\n",
    ").select(['LANGUAGE', 'lang_detected', 'caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different Detected Language\n",
    "\n",
    "df_train.filter(\n",
    "    (pl.col('LANGUAGE') != \"nolang\")\n",
    "    & (pl.col('LANGUAGE') != pl.col('lang_detected'))\n",
    ").select(['LANGUAGE', 'lang_detected', 'caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['is_english'].value_counts()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only English Captions (ground truth == detected)\n",
    "\n",
    "df_train.filter(\n",
    "    (pl.col('LANGUAGE') == 'en')\n",
    "    & (pl.col('lang_detected') == 'en')\n",
    ")['LANGUAGE', 'lang_detected', 'caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detected English Captions (ground truth different from english)\n",
    "\n",
    "df_train.filter(\n",
    "    (pl.col('LANGUAGE') != 'en')\n",
    "    & (pl.col('lang_detected') == 'en')\n",
    ")['LANGUAGE', 'lang_detected', 'caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detected Non-English Captions \n",
    "\n",
    "df_train.filter(\n",
    " (pl.col('lang_detected') != 'en')\n",
    ")['LANGUAGE', 'lang_detected', 'caption']#.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground Truth English Captions Detected as Non-English\n",
    "\n",
    "df_train.filter(\n",
    "    (pl.col('LANGUAGE') == 'en')\n",
    "    & (pl.col('lang_detected') != 'en')\n",
    ")['LANGUAGE', 'lang_detected', 'caption']#.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caption Filtering by Length and Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caption Filtering by Length and Word Count\n",
    "\n",
    "def filter_captions(caption: str, min_words: int=2, min_chars: int=5) -> bool:\n",
    "    # Strip leading/trailing whitespace\n",
    "    cap = caption.strip()\n",
    "    \n",
    "    # Split the caption into words\n",
    "    words = cap.split()\n",
    "    \n",
    "    # Count characters (excluding leading/trailing whitespace)\n",
    "    char_count = len(cap)\n",
    "    \n",
    "    # Check conditions\n",
    "    condition =  len(words) > min_words and char_count > min_chars\n",
    "    \n",
    "    return condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.with_columns(\n",
    "        df_train['caption'].map_elements(\n",
    "            filter_captions, \n",
    "            return_dtype=pl.Boolean, \n",
    "            skip_nulls=False\n",
    "        ).alias('caption_long')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['caption_long'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Dimension and Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def check_image_conditions_from_url(image_url, min_dimension=200, max_aspect_ratio=3):\n",
    "    \"\"\"\n",
    "    Checks if an image from a URL satisfies the given conditions:\n",
    "    - The smaller dimension is above a specified number of pixels.\n",
    "    - The aspect ratio is below a specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "        image_url (str): URL of the image file.\n",
    "        min_dimension (int): Minimum size for the smaller dimension. Default is 200 pixels.\n",
    "        max_aspect_ratio (float): Maximum allowable aspect ratio. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the image satisfies the conditions, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetch the image from the URL\n",
    "        response = requests.get(image_url)\n",
    "        response.raise_for_status()  # Raise an error for bad HTTP responses\n",
    "\n",
    "        # Open the image from the response content\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        width, height = img.size\n",
    "\n",
    "        # Check the smaller dimension\n",
    "        smaller_dimension = min(width, height)\n",
    "        if smaller_dimension < min_dimension:\n",
    "            return False\n",
    "\n",
    "        # Check the aspect ratio\n",
    "        aspect_ratio = max(width / height, height / width)\n",
    "        if aspect_ratio >= max_aspect_ratio:\n",
    "            return False\n",
    "\n",
    "        # Both conditions are satisfied\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image from URL {image_url}: {e}\")\n",
    "        return False\n",
    "\n",
    "def check_image_conditions_from_path(image_path, min_dimension=200, max_aspect_ratio=3):\n",
    "    \"\"\"\n",
    "    Checks if an image satisfies the given conditions:\n",
    "    - The smaller dimension is above a specified number of pixels.\n",
    "    - The aspect ratio is below a specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "        image_path (str): Path to the image file.\n",
    "        min_dimension (int): Minimum size for the smaller dimension. Default is 200 pixels.\n",
    "        max_aspect_ratio (float): Maximum allowable aspect ratio. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the image satisfies the conditions, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the image\n",
    "        with Image.open(image_path) as img:\n",
    "            width, height = img.size\n",
    "\n",
    "        # Check the smaller dimension\n",
    "        smaller_dimension = min(width, height)\n",
    "        if smaller_dimension < min_dimension:\n",
    "            return False\n",
    "\n",
    "        # Check the aspect ratio\n",
    "        aspect_ratio = max(width / height, height / width)\n",
    "        if aspect_ratio >= max_aspect_ratio:\n",
    "            return False\n",
    "\n",
    "        # Both conditions are satisfied\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def check_image_conditions_from_dimensions(width, height, min_dimension=200, max_aspect_ratio=3):\n",
    "    \"\"\"\n",
    "    Checks if an image satisfies the given conditions:\n",
    "    - The smaller dimension is above a specified number of pixels (min_dimension).\n",
    "    - The aspect ratio is below a specified threshold (max_aspect_ratio).\n",
    "\n",
    "    Parameters:\n",
    "        width (int): The width of the image in pixels.\n",
    "        height (int): The height of the image in pixels.\n",
    "        min_dimension (int): Minimum size for the smaller dimension. Default is 200 pixels.\n",
    "        max_aspect_ratio (float): Maximum allowable aspect ratio. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the image satisfies the conditions, False otherwise.\n",
    "    \"\"\"\n",
    "    # Check the smaller dimension\n",
    "    smaller_dimension = min(width, height)\n",
    "    if smaller_dimension < min_dimension:\n",
    "        return False\n",
    "\n",
    "    # Check the aspect ratio\n",
    "    aspect_ratio = max(width / height, height / width)\n",
    "    if aspect_ratio >= max_aspect_ratio:\n",
    "        return False\n",
    "\n",
    "    # Both conditions are satisfied\n",
    "    return True\n",
    "\n",
    "def download_images(url_list, save_folder):\n",
    "    \"\"\"\n",
    "    Downloads images from a list of URLs and saves them to a specified folder.\n",
    "\n",
    "    Parameters:\n",
    "        url_list (list): A list of image URLs.\n",
    "        save_folder (str): The folder where the images will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the folder if it doesn't exist\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    for i, url in enumerate(url_list):\n",
    "        try:\n",
    "            # Get the image data\n",
    "            response = requests.get(url, stream=True)\n",
    "            response.raise_for_status()  # Raise an error for bad HTTP status codes\n",
    "\n",
    "            # Determine the file name and path\n",
    "            file_extension = 'jpg'\n",
    "            file_name = f\"image_{i + 1}.{file_extension}\"\n",
    "            file_path = os.path.join(save_folder, file_name)\n",
    "\n",
    "            # Save the image to the folder\n",
    "            with open(file_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    file.write(chunk)\n",
    "\n",
    "            print(f\"Downloaded: {file_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download image from {url}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "url = df_train['url'].sample(1).to_list()[0]\n",
    "check_image_conditions_from_url(url)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "tot_elapsed_time = ((elapsed_time) * 4e6)/(60 * 60 * 24)\n",
    "print(f\"Esitmated Elapsing Time for 4M rows: {tot_elapsed_time:.2f} Days\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent = os.getcwd()\n",
    "image_dir = 'data'\n",
    "images_path = os.path.join(parent, image_dir)\n",
    "\n",
    "url_list = df_train['url'].head(10).to_list()\n",
    "\n",
    "download_images(url_list, images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for img in os.listdir(images_path):\n",
    "    path = os.path.join(images_path, img)\n",
    "    print(check_image_conditions_from_path(path))\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution Time: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "denominator = len(os.listdir(images_path))\n",
    "tot_elapsed_time = ((elapsed_time/denominator) * 4e6)/60\n",
    "print(f\"Esitmated Elapsing Time for 4M rows: {tot_elapsed_time:.2f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_train = df_train.with_columns(pl.struct(['original_width','original_height']).\n",
    "                        map_elements(\n",
    "                            lambda x: check_image_conditions_from_dimensions(\n",
    "                                            x['original_width'], \n",
    "                                            x['original_height']), \n",
    "                            return_dtype=pl.Boolean, \n",
    "                            skip_nulls=True\n",
    "                        ).alias('image_valid')\n",
    "                    )\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution Time: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "denominator = df_train.shape[0]\n",
    "tot_elapsed_time = ((elapsed_time/denominator) * 4e6)\n",
    "print(f\"Esitmated Elapsing Time for 4M rows: {tot_elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['image_valid'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.filter(pl.col('image_valid') == False).select(\n",
    "    ['original_width', 'original_height', 'image_valid', 'url']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['is_english', 'caption_long', 'image_valid']\\\n",
    "        .to_pandas()\\\n",
    "        .value_counts()\\\n",
    "        .sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PoS\n",
    "\n",
    "Pattern of Speech\n",
    "\n",
    "- SpaCy Tags Meaning: [SpaCy Docs](https://spacy.io/api/token#attributes) -> [Universal POS tags](https://universaldependencies.org/u/pos/)\n",
    "  - `ADJ`: adjective\n",
    "  - `ADP`: adposition\n",
    "  - `ADV`: adverb\n",
    "  - `AUX`: auxiliary\n",
    "  - `CCONJ`: coordinating conjunction\n",
    "  - `DET`: determiner\n",
    "  - `INTJ`: interjection\n",
    "  - `NOUN`: noun\n",
    "  - `NUM`: numeral\n",
    "  - `PART`: particle\n",
    "  - `PRON`: pronoun\n",
    "  - `PROPN`: proper noun\n",
    "  - `PUNCT`: punctuation\n",
    "  - `SCONJ`: subordinating conjunction\n",
    "  - `SYM`: symbol\n",
    "  - `VERB`: verb\n",
    "  - `X`: other\n",
    "- [Universal Dependancy](https://universaldependencies.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficency_model = \"en_core_web_sm\"\n",
    "accuracy_model = \"en_core_web_trf\"\n",
    "\n",
    "# nlp = spacy.load(efficency_model, disable=['lemmatizer', 'ner'])\n",
    "# nlp = spacy.load(efficency_model)\n",
    "nlp = spacy.load(accuracy_model, disable=['lemmatizer', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_caption = df_train.filter(\n",
    "                    (pl.col('is_english') == True)\n",
    "                    & (pl.col('caption_long') == True)\n",
    "                ).with_row_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = random.randint(0, df_eng_caption.shape[0])\n",
    "\n",
    "caption = df_eng_caption.filter(\n",
    "                    pl.col('index') == row_index\n",
    "                )['caption'].to_list()[0]\n",
    "\n",
    "url = df_eng_caption.filter(\n",
    "                    pl.col('index') == row_index\n",
    "                )['url'].to_list()[0]\n",
    "\n",
    "print('\\n',caption,\n",
    "      '\\n',url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(caption)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.require_gpu()\n",
    "# nlp = spacy.load(accuracy_model)\n",
    "\n",
    "nlp = spacy.load(efficency_model)\n",
    "\n",
    "def tag_captions(caption: str, model: str='en_core_web_sm'):\n",
    "    doc = nlp(caption)\n",
    "    tags = [(token.text, token.pos_, token.dep_) for token in doc]\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_caption = df_eng_caption.with_columns(\n",
    "    df_eng_caption['caption'].map_elements(\n",
    "    lambda x: tag_captions(x), \n",
    "    return_dtype=pl.List, \n",
    "    skip_nulls=False).alias('pos_tags')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_concatenate(list_of_lists: list[list[str]]) -> str:\n",
    "    \"\"\"\n",
    "    Extract the second element from each sublist and concatenate them with underscores.\n",
    "\n",
    "    Args:\n",
    "        list_of_lists (list[list[str]]): A list of lists, where each sublist \n",
    "            contains at least two string elements.\n",
    "\n",
    "    Returns:\n",
    "        str: A concatenated string formed by joining the second element of \n",
    "            each sublist with an underscore.\n",
    "\n",
    "    Raises:\n",
    "        IndexError: If any sublist contains fewer than two elements.\n",
    "\n",
    "    Example:\n",
    "        >>> extract_and_concatenate([['a', 'b'], ['c', 'd'], ['e', 'f']])\n",
    "        'b_d_f_'\n",
    "\n",
    "        >>> extract_and_concatenate([['x', 'y'], ['z', 'w']])\n",
    "        'y_w_'\n",
    "    \"\"\"\n",
    "    concatenated_string = \"\"\n",
    "    for element in list_of_lists:\n",
    "        concatenated_string += element[1] + \"_\"\n",
    "    return concatenated_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_caption = df_eng_caption.with_columns(\n",
    "        df_eng_caption['pos_tags'].map_elements(\n",
    "            lambda x: extract_and_concatenate(x),\n",
    "            return_dtype=pl.String,\n",
    "            skip_nulls=False\n",
    "        ).alias('conc_tags')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_caption.filter(\n",
    "    pl.col('index') == 8631\n",
    ")['caption', 'conc_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_caption.filter(\n",
    "    pl.col('index') == 8631\n",
    ")['pos_tags']#.to_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_token_pos(tokens: list[tuple[str, str, str]]) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Count the occurrences of each POS category in a list of token tuples.\n",
    "\n",
    "    Args:\n",
    "        tokens (list[tuple[str, str, str]]): Each tuple should have the format\n",
    "            (token.text, token.pos_, token.dep_).\n",
    "\n",
    "    Returns:\n",
    "        dict[str, int]: A dictionary with POS tags as keys and their counts as values.\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    for token in tokens:\n",
    "        pos = token[1]\n",
    "        counts[pos] = counts.get(pos, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_caption = df_eng_caption.with_columns(\n",
    "    df_eng_caption['pos_tags'].map_elements(\n",
    "        lambda x: count_token_pos(x), \n",
    "        return_dtype=pl.Struct, \n",
    "        skip_nulls=True)\n",
    "        .alias('count_tags')\n",
    ").unnest(\"count_tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_replace = [\n",
    "    'ADJ', 'NOUN', 'PUNCT', 'PROPN', 'VERB', 'SYM', 'NUM', \n",
    "    'PART', 'SCONJ', 'ADP', 'DET', 'PRON', 'SPACE', 'CCONJ', \n",
    "    'INTJ', 'X', 'AUX', 'ADV'\n",
    "]\n",
    "\n",
    "# Replace null values in the selected columns only\n",
    "df_eng_caption = df_eng_caption.with_columns([\n",
    "    pl.col(col).fill_null(0) for col in columns_to_replace\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def create_bar_plots(\n",
    "        df : pl.DataFrame, \n",
    "        columns_to_plot: list[str], \n",
    "        num_cols: int,\n",
    "        graph_heihgt: int=600,\n",
    "    ):\n",
    "\n",
    "    num_graphs = len(columns_to_plot)\n",
    "\n",
    "    num_rows = math.ceil(num_graphs/num_cols)\n",
    "\n",
    "    fig = make_subplots(\n",
    "                        rows=num_rows, \n",
    "                        cols=num_cols,\n",
    "                        shared_xaxes=False,\n",
    "                        vertical_spacing=0.03,\n",
    "                        # subplot_titles=columns_to_plot\n",
    "                        )\n",
    "\n",
    "    i = 0\n",
    "    for k in range(1, num_cols + 1):\n",
    "        for j in range(1, num_rows+1):\n",
    "            if i < num_graphs:\n",
    "                field = columns_to_plot[i]\n",
    "\n",
    "                fig.add_trace(\n",
    "                    go.Bar(\n",
    "                        x=df[field]\\\n",
    "                            .value_counts()\\\n",
    "                            .sort('count', descending=False)[field]\n",
    "                            .to_list(),\n",
    "\n",
    "                        y=df[field]\\\n",
    "                            .value_counts()\\\n",
    "                            .sort('count', descending=False)['count']\n",
    "                            .to_list(),\n",
    "\n",
    "                        orientation='v',\n",
    "                        showlegend=True,\n",
    "                        # legendgroup=field,\n",
    "                        name=field\n",
    "                        ),\n",
    "                    row=j, col=k\n",
    "                )\n",
    "\n",
    "                fig.add_annotation(\n",
    "                    text=field,\n",
    "                    xref=\"x2 domain\", yref=\"y2 domain\",\n",
    "                    x=0.5, y=1.1,\n",
    "                    showarrow=False,\n",
    "                    row=j, col=k\n",
    "                )\n",
    "                i += 1\n",
    "\n",
    "    fig.update_layout(height=600*num_rows, width=600*num_cols)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_plots(df_eng_caption, columns_to_replace, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- senza aggettivi\n",
    "- con numeri\n",
    "- con punteggiatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_caption.filter(\n",
    "    pl.col('ADJ') == 0\n",
    ")['caption', 'conc_tags', 'ADJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_caption.filter(\n",
    "    pl.col('NOUN') == 0\n",
    ")['caption', 'conc_tags', 'NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_caption.filter(\n",
    "    (pl.col('ADJ') != 0)\n",
    "    & (pl.col('NOUN') != 0)\n",
    "    & (pl.col('NUM') == 0)\n",
    ")['caption', 'conc_tags', 'ADJ', 'NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_tags(tags: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the unique tags from a concatenated string of tags.\n",
    "\n",
    "    Args:\n",
    "        tags (str): A string with tags concatenated by underscores.\n",
    "\n",
    "    Returns:\n",
    "        str: A string with unique tags concatenated by underscores.\n",
    "    \"\"\"\n",
    "    unique_tags = set(tags.split('_')[:-1])\n",
    "    unique_tags = sorted(list(unique_tags))\n",
    "    return \"_\".join(unique_tags)\n",
    "\n",
    "def count_unique_tags(tags: str) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of unique tags in a concatenated string of tags.\n",
    "\n",
    "    Args:\n",
    "        tags (str): A string with tags concatenated by underscores.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of unique tags in the string.\n",
    "    \"\"\"\n",
    "    unique_tags = extract_unique_tags(tags)\n",
    "    return len(unique_tags.split('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice = random.randint(0, df_eng_caption.shape[0])\n",
    "\n",
    "original = df_eng_caption.filter(\n",
    "    pl.col('index') == indice\n",
    ")['conc_tags'].to_list()[0]\n",
    "\n",
    "unique_sorted_caption = extract_unique_tags(original)\n",
    "tag_num = count_unique_tags(original)\n",
    "\n",
    "print(\n",
    "    f\"Original Tags:\\n{original}\\n\\n\"\n",
    "    f\"Unique Tags:\\n{unique_sorted_caption}\"\n",
    "    f\"\\n\\nNumber of Unique Tags: {tag_num}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_caption = df_eng_caption.with_columns(\n",
    "    df_eng_caption['conc_tags'].map_elements(\n",
    "        lambda x: extract_unique_tags(x), \n",
    "        return_dtype=pl.String, \n",
    "        skip_nulls=True)\n",
    "        .alias('unique_tags')\n",
    ")\n",
    "\n",
    "df_eng_caption = df_eng_caption.with_columns(\n",
    "    df_eng_caption['conc_tags'].map_elements(\n",
    "        lambda x: count_unique_tags(x), \n",
    "        return_dtype=pl.Int8, \n",
    "        skip_nulls=True)\n",
    "        .alias('count_unique_tags')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_caption['unique_tags'].value_counts().sort('count', descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_plots(df_eng_caption, ['count_unique_tags'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCC PoS distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcc_train = pl.read_csv(\n",
    "        'data/Train_GCC-training.tsv',\n",
    "        separator='\\t',\n",
    "        new_columns = ['caption', 'url'],\n",
    "        schema = {\n",
    "            'captions': pl.String,\n",
    "            'url': pl.String\n",
    "        }\n",
    "    )\n",
    "\n",
    "gcc_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcc_val = pl.read_csv(\n",
    "#         'data/Validation_GCC-1.1.0-Validation.tsv',\n",
    "#         separator='\\t',\n",
    "#         new_columns = ['caption', 'url'],\n",
    "#         schema = {\n",
    "#             'captions': pl.String,\n",
    "#             'url': pl.String\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "# gcc_val.shape\n",
    "\n",
    "# gcc_train_image_label = pl.read_csv(\n",
    "#         'data/Image_Labels_Subset_Train_GCC-Labels-training.tsv',\n",
    "#         separator='\\t',\n",
    "#         has_header=False\n",
    "# )[:, :-2]\n",
    "\n",
    "# gcc_train_image_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = gcc_train['caption'].is_duplicated()\n",
    "\n",
    "gcc_train_duplicated = gcc_train.filter(\n",
    "    condition\n",
    ").sort('caption').with_row_index()\n",
    "\n",
    "gcc_train_duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_captions = gcc_train.filter(\n",
    "    pl.col('caption').is_duplicated()\n",
    ")['caption'].unique().to_list()\n",
    "\n",
    "\n",
    "gcc_train_clean = gcc_train.filter(\n",
    "    ~pl.col(\"caption\").is_in(duplicated_captions)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcc_train_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcc_train_clean = gcc_train_clean.with_columns(\n",
    "    gcc_train_clean['caption'].map_elements(\n",
    "        lambda x: is_english_sentence(x), \n",
    "        return_dtype=map_data_types, \n",
    "        skip_nulls=True)\n",
    "        .alias('lang_detection')\n",
    ").unnest(\"lang_detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcc_train_clean['is_english'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcc_train_clean.filter(\n",
    "    (pl.col('is_english') == True)\n",
    "    & (pl.col('score') > 0.9)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcc_train_clean = gcc_train_clean.with_columns(\n",
    "        gcc_train_clean['caption'].map_elements(\n",
    "            filter_captions, \n",
    "            return_dtype=pl.Boolean, \n",
    "            skip_nulls=False\n",
    "        ).alias('caption_long')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcc_train_clean['caption_long'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcc_eng_caption = gcc_train_clean.filter(\n",
    "                    (pl.col('is_english') == True)\n",
    "                    & (pl.col('caption_long') == True)\n",
    "                     & (pl.col('score') > 0.9)\n",
    "                ).with_row_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcc_eng_caption = gcc_eng_caption.with_columns(\n",
    "    gcc_eng_caption['caption'].str.strip_chars_end(' .')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_columns(df: pl.DataFrame, column_list: list[str]) -> pl.DataFrame:\n",
    "    # Find columns that are not present in the dataframe\n",
    "    missing_columns = [col for col in column_list if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        # Create expressions that add each missing column as null values\n",
    "        df = df.with_columns(\n",
    "            [pl.lit(None).alias(col) for col in missing_columns]\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.require_gpu()\n",
    "nlp = spacy.load(accuracy_model)\n",
    "\n",
    "# nlp = spacy.load(efficency_model)\n",
    "\n",
    "\n",
    "gcc_eng_caption = gcc_eng_caption.with_columns(\n",
    "    gcc_eng_caption['caption'].map_elements(\n",
    "    lambda x: tag_captions(x), \n",
    "    return_dtype=pl.List, \n",
    "    skip_nulls=False).alias('pos_tags')\n",
    ")\n",
    "\n",
    "gcc_eng_caption = gcc_eng_caption.with_columns(\n",
    "        gcc_eng_caption['pos_tags'].map_elements(\n",
    "            lambda x: extract_and_concatenate(x),\n",
    "            return_dtype=pl.String,\n",
    "            skip_nulls=False\n",
    "        ).alias('conc_tags')\n",
    "    )\n",
    "\n",
    "gcc_eng_caption = gcc_eng_caption.with_columns(\n",
    "    gcc_eng_caption['pos_tags'].map_elements(\n",
    "        lambda x: count_token_pos(x), \n",
    "        return_dtype=pl.Struct, \n",
    "        skip_nulls=True)\n",
    "        .alias('count_tags')\n",
    ").unnest(\"count_tags\")\n",
    "\n",
    "columns_to_replace = [\n",
    "    'ADJ', 'NOUN', 'PUNCT', 'PROPN', 'VERB', 'SYM', 'NUM', \n",
    "    'PART', 'SCONJ', 'ADP', 'DET', 'PRON', 'SPACE', 'CCONJ', \n",
    "    'INTJ', 'X', 'AUX', 'ADV'\n",
    "]\n",
    "\n",
    "gcc_eng_caption = add_missing_columns(gcc_eng_caption, columns_to_replace)\n",
    "\n",
    "# Replace null values in the selected columns only\n",
    "gcc_eng_caption = gcc_eng_caption.with_columns([\n",
    "    pl.col(col).fill_null(0) for col in columns_to_replace\n",
    "])\n",
    "\n",
    "gcc_eng_caption = gcc_eng_caption.with_columns(\n",
    "    gcc_eng_caption['conc_tags'].map_elements(\n",
    "        lambda x: extract_unique_tags(x), \n",
    "        return_dtype=pl.String, \n",
    "        skip_nulls=True)\n",
    "        .alias('unique_tags')\n",
    ")\n",
    "\n",
    "gcc_eng_caption = gcc_eng_caption.with_columns(\n",
    "    gcc_eng_caption['conc_tags'].map_elements(\n",
    "        lambda x: count_unique_tags(x), \n",
    "        return_dtype=pl.Int8, \n",
    "        skip_nulls=True)\n",
    "        .alias('count_unique_tags')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "# gcc_eng_caption.write_parquet('data/gcc_eng_caption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "gcc_eng_caption = pl.read_parquet('data/gcc_eng_caption')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutterstock PoS distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutterstock = pl.read_csv('data/shutterstock.csv', \n",
    "            separator='\\t',\n",
    "            new_columns = ['url', 'caption'],\n",
    "            schema = {\n",
    "                'url': pl.String,\n",
    "                'captions': pl.String\n",
    "            }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = shutterstock['caption'].is_duplicated()\n",
    "\n",
    "shutterstock_duplicated = shutterstock.filter(\n",
    "    condition\n",
    ").sort('caption').with_row_index()\n",
    "\n",
    "shutterstock_duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_captions = shutterstock.filter(\n",
    "    pl.col('caption').is_duplicated()\n",
    ")['caption'].unique().to_list()\n",
    "\n",
    "\n",
    "shutterstock_clean = shutterstock.filter(\n",
    "    ~pl.col(\"caption\").is_in(duplicated_captions)\n",
    "    )\n",
    "\n",
    "shutterstock_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutterstock_clean = shutterstock_clean.with_columns(\n",
    "    shutterstock_clean['caption'].map_elements(\n",
    "        lambda x: is_english_sentence(x), \n",
    "        return_dtype=map_data_types, \n",
    "        skip_nulls=True)\n",
    "        .alias('lang_detection')\n",
    ").unnest(\"lang_detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutterstock_clean['is_english'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutterstock_clean.filter(\n",
    "    (pl.col('is_english') == True)\n",
    "    & (pl.col('score') > 0.9)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutterstock_clean = shutterstock_clean.with_columns(\n",
    "        shutterstock_clean['caption'].map_elements(\n",
    "            filter_captions, \n",
    "            return_dtype=pl.Boolean, \n",
    "            skip_nulls=False\n",
    "        ).alias('caption_long')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutterstock_clean['caption_long'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutterstock_clean = shutterstock_clean.filter(\n",
    "                    (pl.col('is_english') == True)\n",
    "                    & (pl.col('caption_long') == True)\n",
    "                     & (pl.col('score') > 0.9)\n",
    "                ).with_row_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutterstock_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.require_gpu()\n",
    "nlp = spacy.load(accuracy_model)\n",
    "\n",
    "# nlp = spacy.load(efficency_model)\n",
    "\n",
    "\n",
    "shutterstock_clean = shutterstock_clean.with_columns(\n",
    "    shutterstock_clean['caption'].map_elements(\n",
    "    lambda x: tag_captions(x), \n",
    "    return_dtype=pl.List, \n",
    "    skip_nulls=False).alias('pos_tags')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutterstock_clean = shutterstock_clean.with_columns(\n",
    "        shutterstock_clean['pos_tags'].map_elements(\n",
    "            lambda x: extract_and_concatenate(x),\n",
    "            return_dtype=pl.String,\n",
    "            skip_nulls=False\n",
    "        ).alias('conc_tags')\n",
    "    )\n",
    "\n",
    "shutterstock_clean = shutterstock_clean.with_columns(\n",
    "    shutterstock_clean['pos_tags'].map_elements(\n",
    "        lambda x: count_token_pos(x), \n",
    "        return_dtype=pl.Struct, \n",
    "        skip_nulls=True)\n",
    "        .alias('count_tags')\n",
    ").unnest(\"count_tags\")\n",
    "\n",
    "columns_to_replace = [\n",
    "    'ADJ', 'NOUN', 'PUNCT', 'PROPN', 'VERB', 'SYM', 'NUM', \n",
    "    'PART', 'SCONJ', 'ADP', 'DET', 'PRON', 'SPACE', 'CCONJ', \n",
    "    'INTJ', 'X', 'AUX', 'ADV'\n",
    "]\n",
    "\n",
    "shutterstock_clean = add_missing_columns(\n",
    "        shutterstock_clean, \n",
    "        columns_to_replace\n",
    "    )\n",
    "\n",
    "# Replace null values in the selected columns only\n",
    "shutterstock_clean = shutterstock_clean.with_columns([\n",
    "    pl.col(col).fill_null(0) for col in columns_to_replace\n",
    "])\n",
    "\n",
    "shutterstock_clean = shutterstock_clean.with_columns(\n",
    "    shutterstock_clean['conc_tags'].map_elements(\n",
    "        lambda x: extract_unique_tags(x), \n",
    "        return_dtype=pl.String, \n",
    "        skip_nulls=True)\n",
    "        .alias('unique_tags')\n",
    ")\n",
    "\n",
    "shutterstock_clean = shutterstock_clean.with_columns(\n",
    "    shutterstock_clean['conc_tags'].map_elements(\n",
    "        lambda x: count_unique_tags(x), \n",
    "        return_dtype=pl.Int8, \n",
    "        skip_nulls=True)\n",
    "        .alias('count_unique_tags')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutterstock_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "# shutterstock_clean.write_parquet('data/shutterstock_eng_caption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "shutterstock_clean = pl.read_parquet('data/shutterstock_eng_caption')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=gcc_eng_caption['count_unique_tags'],\n",
    "    name='GCC Dataset',\n",
    "    opacity=0.75,\n",
    "    nbinsx=20\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=shutterstock_clean['count_unique_tags'],\n",
    "    name='Shutterstock Dataset',\n",
    "    opacity=0.75,\n",
    "    nbinsx=20\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Distribution of Unique POS Tags Count',\n",
    "    xaxis_title='Number of Unique POS Tags',\n",
    "    yaxis_title='Count',\n",
    "    barmode='overlay',\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of samples for each dataset\n",
    "gcc_total = len(gcc_eng_caption)\n",
    "shutterstock_total = len(shutterstock_clean)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add normalized histograms\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=gcc_eng_caption['count_unique_tags'],\n",
    "    name='GCC Dataset',\n",
    "    opacity=0.75,\n",
    "    nbinsx=20,\n",
    "    histnorm='percent'  # Normalize to percentage\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=shutterstock_clean['count_unique_tags'],\n",
    "    name='Shutterstock Dataset',\n",
    "    opacity=0.75,\n",
    "    nbinsx=20,\n",
    "    histnorm='percent'  # Normalize to percentage\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Distribution of Unique POS Tags Count (Normalized)',\n",
    "    xaxis_title='Number of Unique POS Tags',\n",
    "    yaxis_title='Percentage',\n",
    "    barmode='overlay',\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique patterns from each dataset\n",
    "gcc_patterns = set(gcc_eng_caption['unique_tags'].to_list())\n",
    "shutterstock_patterns = set(shutterstock_clean['unique_tags'].to_list())\n",
    "\n",
    "# Find patterns that appear in both datasets\n",
    "common_patterns = gcc_patterns.intersection(shutterstock_patterns)\n",
    "\n",
    "# Find patterns unique to each dataset\n",
    "gcc_only = gcc_patterns - shutterstock_patterns\n",
    "shutterstock_only = shutterstock_patterns - gcc_patterns\n",
    "\n",
    "print(f\"Total patterns in GCC: {len(gcc_patterns)}\")\n",
    "print(f\"Total patterns in Shutterstock: {len(shutterstock_patterns)}\")\n",
    "print(f\"Common patterns: {len(common_patterns)}\")\n",
    "print(f\"Patterns only in GCC: {len(gcc_only)}\")\n",
    "print(f\"Patterns only in Shutterstock: {len(shutterstock_only)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of samples for each dataset\n",
    "gcc_total = len(gcc_eng_caption)\n",
    "shutterstock_total = len(shutterstock_clean)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add normalized histograms\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=gcc_eng_caption.filter(\n",
    "        pl.col('unique_tags').is_in(common_patterns)\n",
    "    )['count_unique_tags'],\n",
    "    name='GCC Dataset',\n",
    "    opacity=0.75,\n",
    "    nbinsx=20,\n",
    "    histnorm='percent'  # Normalize to percentage\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=shutterstock_clean.filter(\n",
    "        pl.col('unique_tags').is_in(common_patterns)\n",
    "    )['count_unique_tags'],\n",
    "    name='Shutterstock Dataset',\n",
    "    opacity=0.75,\n",
    "    nbinsx=20,\n",
    "    histnorm='percent'  # Normalize to percentage\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Distribution of Unique POS Tags Count (Normalized) - Common Patterns',\n",
    "    xaxis_title='Number of Unique POS Tags',\n",
    "    yaxis_title='Percentage',\n",
    "    barmode='overlay',\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of samples for each dataset\n",
    "gcc_total = len(gcc_eng_caption)\n",
    "shutterstock_total = len(shutterstock_clean)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add normalized histograms\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=gcc_eng_caption.filter(\n",
    "        pl.col('unique_tags').is_in(gcc_only)\n",
    "    )['count_unique_tags'],\n",
    "    name='GCC Dataset',\n",
    "    opacity=0.75,\n",
    "    nbinsx=20,\n",
    "    histnorm='percent'  # Normalize to percentage\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=shutterstock_clean.filter(\n",
    "        pl.col('unique_tags').is_in(shutterstock_only)\n",
    "    )['count_unique_tags'],\n",
    "    name='Shutterstock Dataset',\n",
    "    opacity=0.75,\n",
    "    nbinsx=20,\n",
    "    histnorm='percent'  # Normalize to percentage\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Distribution of Unique POS Tags Count (Normalized) - Unique Patterns',\n",
    "    xaxis_title='Number of Unique POS Tags',\n",
    "    yaxis_title='Percentage',\n",
    "    barmode='overlay',\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Punct, Zero Noun, Zero Adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcc_eng_caption_cleaned = gcc_eng_caption.filter(\n",
    "    (pl.col('ADJ') != 0)\n",
    "    & (pl.col('NOUN') != 0)\n",
    "    & (pl.col('NUM') == 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutterstock_cleaned = shutterstock_clean.filter(\n",
    "    (pl.col('ADJ') != 0)\n",
    "    & (pl.col('NOUN') != 0)\n",
    "    & (pl.col('NUM') == 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=gcc_eng_caption_cleaned['count_unique_tags'],\n",
    "    name='GCC Dataset',\n",
    "    opacity=0.75,\n",
    "    nbinsx=20\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=shutterstock_cleaned['count_unique_tags'],\n",
    "    name='Shutterstock Dataset',\n",
    "    opacity=0.75,\n",
    "    nbinsx=20\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Distribution of Unique POS Tags Count',\n",
    "    xaxis_title='Number of Unique POS Tags',\n",
    "    yaxis_title='Count',\n",
    "    barmode='overlay',\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique patterns from each dataset\n",
    "gcc_patterns = set(gcc_eng_caption_cleaned['unique_tags'].to_list())\n",
    "shutterstock_patterns = set(shutterstock_cleaned['unique_tags'].to_list())\n",
    "\n",
    "# Find patterns that appear in both datasets\n",
    "common_patterns = gcc_patterns.intersection(shutterstock_patterns)\n",
    "\n",
    "# Find patterns unique to each dataset\n",
    "gcc_only = gcc_patterns - shutterstock_patterns\n",
    "shutterstock_only = shutterstock_patterns - gcc_patterns\n",
    "\n",
    "print(f\"Total patterns in GCC: {len(gcc_patterns)}\")\n",
    "print(f\"Total patterns in Shutterstock: {len(shutterstock_patterns)}\")\n",
    "print(f\"Common patterns: {len(common_patterns)}\")\n",
    "print(f\"Patterns only in GCC: {len(gcc_only)}\")\n",
    "print(f\"Patterns only in Shutterstock: {len(shutterstock_only)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of samples for each dataset\n",
    "gcc_total = len(gcc_eng_caption)\n",
    "shutterstock_total = len(shutterstock_clean)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add normalized histograms\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=gcc_eng_caption_cleaned.filter(\n",
    "        pl.col('unique_tags').is_in(common_patterns)\n",
    "    )['count_unique_tags'],\n",
    "    name='GCC Dataset',\n",
    "    opacity=0.75,\n",
    "    nbinsx=20,\n",
    "    histnorm='percent'  # Normalize to percentage\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=shutterstock_cleaned.filter(\n",
    "        pl.col('unique_tags').is_in(common_patterns)\n",
    "    )['count_unique_tags'],\n",
    "    name='Shutterstock Dataset',\n",
    "    opacity=0.75,\n",
    "    nbinsx=20,\n",
    "    histnorm='percent'  # Normalize to percentage\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Distribution of Unique POS Tags Count (Normalized) - Common Patterns',\n",
    "    xaxis_title='Number of Unique POS Tags',\n",
    "    yaxis_title='Percentage',\n",
    "    barmode='overlay',\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save common patterns to a text file\n",
    "with open('common_pos_patterns.txt', 'w') as f:\n",
    "    for pattern in sorted(common_patterns):\n",
    "        f.write(pattern + '\\n')\n",
    "\n",
    "print(f\"Saved {len(common_patterns)} patterns to common_pos_patterns.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the common_patterns set as a list to a pickle file\n",
    "with open('common_patterns.txt', 'w') as f:\n",
    "    f.write(list(common_patterns), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
